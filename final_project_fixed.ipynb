{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/hindhammad24/Final-Project/blob/main/final_project_fixed.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "URt4Q-TNmNDj"
      },
      "source": [
        "# **NutriGuide** - Health Nutrition RAG Chatbot\n",
        "**By Hind Hammad and Manmeet Sagri**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZIi4bjh7mIJd"
      },
      "source": [
        "**Project Overview**\n",
        "\n",
        "In our project, we are creating NutriGuide, a Retrieval-Augmented Generation (RAG) chatbot designed to give safe, evidence-based nutrition guidance. We built this system because we wanted to create a tool that doesn’t rely on random internet information, but instead pulls directly from trusted health sources like Canada’s Food Guide and the World Health Organization. Our chatbot uses retrieval to gather verified information and a language model to summarize it into clear, simple responses. Since we are focusing on general healthy eating, our system avoids giving medical or extreme diet advice and instead supports users with educational nutrition information.\n",
        "\n",
        "**Problem**\n",
        "\n",
        "While working on this project, we realized how difficult it is for people to access reliable nutrition information online. Many websites, influencers, and social media platforms share conflicting or unsafe diet advice, which can easily confuse users. A lot of people end up relying on viral trends instead of evidence-based recommendations. We wanted to solve this problem by building a chatbot that only uses trustworthy public-health information and provides consistent, safe guidance.\n",
        "\n",
        "**significance**\n",
        "\n",
        "As we researched different ideas, we quickly saw how much misinformation exists in the nutrition space. This problem felt meaningful to us because we noticed that students, newcomers to Canada, and busy individuals often struggle to identify what is actually healthy or safe. Since we are working with official health sources, our project gives users a way to access dependable information without having to navigate complex government websites. To us, the significance of this project lies in making nutrition education more accessible, culturally neutral, and easier to understand.\n",
        "\n",
        "**Users/Usecase**\n",
        "\n",
        "From our perspective, the people who will benefit most from our chatbot are users who want quick, general nutrition guidance without dealing with confusing or unreliable online sources. This includes beginners learning about healthy eating, people who want simple explanations, and anyone who feels overwhelmed by online diet content. We designed the use case to be a straightforward chat interface where users type questions and receive safe, evidence-based responses grounded directly in our dataset.\n",
        "\n",
        "**where/how they'll be using the model?**\n",
        "\n",
        "We expect users to interact with our model through a lightweight chatbot interface, either on a web app or through an interactive notebook. Our goal is for people to use it in everyday situations; for example, when planning meals, trying to understand healthy habits, or wanting quick reminders about balanced eating. Since the model retrieves real text from reliable sources, users get accurate information instantly without needing to browse multiple websites.\n",
        "\n",
        "**what kind of data it requires?**\n",
        "\n",
        "To build this system, we needed data that is both reliable and allowed for academic use. We collected text from Canada’s Food Guide and WHO Healthy Diet resources because these are publicly available educational materials. We cleaned the text, split it into smaller chunks; and turned it into embeddings so the chatbot can retrieve the right information when a user asks a question. Our model does not collect or store personal user data, it only uses the user’s question to retrieve the correct evidence and generate an answer.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4Qpcib1lo92R"
      },
      "source": [
        "# **Model Source and How We Used It**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kKCFdkZsp3AM"
      },
      "source": [
        "For this project, we began by forking the RAG (Retrieval-Augmented Generation) notebook provided by our TA. The original notebook (“Retrieval_Augmented_Generation_(RAG).ipynb”) contained a minimal working example of a RAG pipeline using HuggingFace models. We used this notebook as the foundation for our system and adapted it to our nutrition use case.\n",
        "\n",
        "Inside the instructor’s notebook, two external models were loaded through the HuggingFace transformers and sentence-transformers libraries:\n",
        "\n",
        "- DistilGPT-2 (text generation)\n",
        "\n",
        "- all-MiniLM-L6-v2 (sentence embeddings)\n",
        "\n",
        "\n",
        "\n",
        " ### **DistilGPT-2**\n",
        "\n",
        "Source: HuggingFace model hub\n",
        "\n",
        "Original Repository: https://huggingface.co/distilgpt2\n",
        "\n",
        "License: MIT License (allows modification, reuse, and non-commercial/educational deployment)\n",
        "\n",
        "**How we used it:**\n",
        "\n",
        "T- he instructor’s notebook loaded DistilGPT-2 using the HuggingFace pipeline(\"text-generation\").\n",
        "\n",
        "- We kept this part of the code but modified the prompt template so that the model only generates answers using text from our nutrition dataset.\n",
        "\n",
        "- We added safety rules (no diagnoses, no calories, no meal plans, etc.) and included them in the system prompt.\n",
        "\n",
        "- DistilGPT-2 then generates short, coherent answers grounded in retrieved evidence.\n",
        "\n",
        "### **MiniLM-L6-v2**\n",
        "\n",
        "Source: SentenceTransformers library\n",
        "\n",
        "Original Repository: https://huggingface.co/sentence-transformers/all-MiniLM-L6-v2\n",
        "\n",
        "License: Apache 2.0 License (permits research use, modification, and redistribution)\n",
        "\n",
        "**How we used it**\n",
        "\n",
        "- In the instructor’s notebook, this model was used to convert the dummy RAG documents into 384-dimensional embeddings.\n",
        "\n",
        "- We replaced the original placeholder texts with our own dataset, which we created using reliable public health sources (Canada’s Food Guide and WHO Health Diet Facts).\n",
        "\n",
        "- The embedding model allowed us to perform semantic search and retrieve the top relevant pieces of nutrition evidence for each user question.\n",
        "\n",
        "### **FAISS Index**\n",
        "\n",
        "Source: Installed inside the instructor’s notebook (FAISS CPU version)\n",
        "\n",
        "License: MIT License\n",
        "\n",
        "Repository: https://github.com/facebookresearch/faiss\n",
        "\n",
        "**How we used it:**\n",
        "\n",
        "- FAISS was originally used in the instructor’s example to store embeddings of small RAG documents.\n",
        "\n",
        "- We kept the same retrieval structure, but instead of the original sample RAG text, we inserted nutrition text chunks.\n",
        "\n",
        "- This enabled accurate retrieval of relevant information before passing it to the language model.\n",
        "\n",
        "### **Instructor Notebook**\n",
        "\n",
        "Provided by: TA\n",
        "\n",
        "Filename: Retrieval_Augmented_Generation_(RAG).ipynb\n",
        "\n",
        "**How we used and modified it:**\n",
        "\n",
        "We forked the entire notebook as the starting point for our project.\n",
        "\n",
        "We replaced the example documents = [...] list with our nutrition dataset.\n",
        "\n",
        "We rewrote the generate_with_rag() function to:\n",
        "\n",
        "- enforce safety guardrails,\n",
        "\n",
        "- inject public-health context,\n",
        "\n",
        "- prevent unsafe diet or medical advice,\n",
        "\n",
        "- cite retrieved information clearly.\n",
        "\n",
        "- We added a testing framework using real-world diet questions.\n",
        "\n",
        "- We integrated everything into a simple chatbot interface for deployment."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L3FGskainy8v"
      },
      "source": [
        "# **RAG Dataset Description**\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0vWhx34xdrPh"
      },
      "source": [
        "## **Overview of the Dataset**\n",
        "\n",
        "For this project, we created a custom RAG dataset focused on evidence-based nutrition and healthy eating practices. Our goal was to ensure that NutriGuide provides safe, factual, and research-supported information rather than relying on general internet knowledge. The dataset contains information from trusted public-health sources, including Government of Canada dietary guidelines, the World Health Organization (WHO), and the Heart & Stroke Foundation. All documents focus on general healthy eating, nutrient recommendations, dietary patterns, and practical guidance for the public.\n",
        "\n",
        "The final dataset contains 34 cleaned text files categorized into themes such as vegetables and fruits, whole grains, protein foods, food skills, food marketing, vitamin D, life-stage nutrition, dietary shifts, healthy eating habits, and global nutrition recommendations.\n",
        "\n",
        "## **Data Collection Process & Source Links**\n",
        "\n",
        "We manually collected the dataset from publicly available, non-copyrighted educational resources intended for public use. All content came from official health organizations that allow non-commercial academic reuse.\n",
        "\n",
        "**Main sources used:**\n",
        "\n",
        "Canada’s Food Guide – Government of Canada\n",
        "https://food-guide.canada.ca/en/\n",
        "\n",
        "Canada’s Dietary Guidelines – Health Canada (PDF)\n",
        "https://www.canada.ca/content/dam/hc-sc/documents/services/food-nutrition/canada-food-guide/resources/canada-food-guide-backgrounder/food-guide-snapshot.pdf\n",
        "\n",
        "World Health Organization: Healthy Diet\n",
        "https://www.who.int/news-room/fact-sheets/detail/healthy-diet\n",
        "\n",
        "Heart & Stroke Foundation – Healthy Eating Basics\n",
        "https://www.heartandstroke.ca/healthy-living/healthy-eating/healthy-eating-basics\n",
        "\n",
        "## **How we collected the data**\n",
        "\n",
        "- We manually copied sections of text from each source.\n",
        "\n",
        "- We ensured only public information intended for public education was included.\n",
        "\n",
        "- We avoided copyrighted recipes, menus, or branded content.\n",
        "\n",
        "- We grouped text into logical sections (e.g., vegetables, protein foods, food marketing).\n",
        "\n",
        "**How we stored the dataset**\n",
        "\n",
        "After cleaning each text section, we saved them as individual .txt files and packaged them into a folder, which we uploaded as data.zip for use in our RAG notebook.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z_jGywr11D9R"
      },
      "source": [
        "# **Data Cleaning & Preprocessing**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TdGzgb_Iym3Y"
      },
      "source": [
        "The raw text from each website or PDF contained many elements that would interfere with retrieval (menus, buttons, duplicate headings, references, formatting noise). To make the dataset consistent and useful for RAG, we cleaned all text manually.\n",
        "\n",
        "## **Cleaning steps we applied**\n",
        "\n",
        "- Removed HTML menus, navigation text, sidebars, “On this page,” and repeated headings.\n",
        "\n",
        "- Removed unnecessary lists (e.g., “Twitter / Facebook / Share”).\n",
        "\n",
        "- Removed references, footnotes, citation numbers, and academic formatting.\n",
        "\n",
        "- Consolidated bullet lists into readable sentences.\n",
        "\n",
        "- Paraphrased and rewrote large portions for clarity while preserving the original meaning.\n",
        "\n",
        "- Removed medical advice, prescriptive diet plans, and anything unsafe or diagnostic.\n",
        "\n",
        "- Split long sections into smaller, topic-based files to improve retrieval accuracy.\n",
        "\n",
        "- Ensured all wording was neutral, educational, and general, not personalized.\n",
        "\n",
        "- Ensured no copyrighted text from non-governmental sources was used verbatim.\n",
        "\n",
        "- All cleaned text files contain short, self-contained paragraphs that the embedding model can index effectively.\n",
        "\n",
        "\n",
        "# **Naming Convention and File Organization**\n",
        "\n",
        "To keep the dataset easy to load, search, and maintain, we used a consistent naming system:\n",
        "\n",
        "**All files are saved as:** source_topic.txt\n",
        "\n",
        "Examples:\n",
        "\n",
        "- canada_vegetables_and_fruits.txt\n",
        "\n",
        "- canada_whole_grains.txt\n",
        "\n",
        "- canada_protein_foods.txt\n",
        "\n",
        "- canada_mindful_eating.txt\n",
        "\n",
        "- who_healthy_diet_key_facts.txt\n",
        "\n",
        "- who_practical_healthy_eating_tips.txt\n",
        "\n",
        "- heartstroke_healthy_eating_basics.txt\n",
        "\n",
        "\n",
        "**Naming rules we followed**\n",
        "\n",
        "- Lowercase only\n",
        "- Words separated by underscores\n",
        "- Prefix shows the source (canada, who, heartstroke)\n",
        "\n",
        "- Second part describes the topic\n",
        "\n",
        "- One topic per file for cleaner chunking\n",
        "\n",
        "The dataset folder contains all .txt files and is stored as data/ in our project directory. We zipped the folder (data.zip) for submission and to make it easy to load inside the Jupyter notebook."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A241w5TIUs--"
      },
      "source": [
        "# **Issues and Biases**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rC6TdC8qiCAV"
      },
      "source": [
        "## **Is the Dataset Balanced?**\n",
        "\n",
        "Our dataset is reasonably balanced across major nutrition themes because we intentionally included information from multiple trusted sources (Canada’s Food Guide, WHO, Heart & Stroke Foundation). The dataset covers vegetables and fruits, whole grains, protein foods, healthy fats, water, food skills, processed foods, dietary shifts, life-stage nutrition, and supplementation. Each category has several text files, which allows the retrieval system to access information from different perspectives. While it is not perfectly uniform, the dataset includes a wide enough distribution of topics to support reliable retrieval for general healthy-eating questions.\n",
        "\n",
        "\n",
        "## **What Is Missing From the Dataset**\n",
        "\n",
        "Although the dataset is comprehensive for general public-health nutrition, several areas are intentionally not included:\n",
        "\n",
        "- No medical or clinical nutrition information (e.g., diabetes-specific diets, kidney disease diets)\n",
        "\n",
        "- No personalized diet plans, calorie targets, or macro calculations\n",
        "\n",
        "- No weight-loss programs, therapeutic diets (keto, low-carb, intermittent fasting)\n",
        "\n",
        "- No fitness or bodybuilding nutrition\n",
        "\n",
        "- No recipes or cooking instructions from copyrighted sources\n",
        "\n",
        "- No cultural dietary guidelines beyond general statements\n",
        "\n",
        "- No Indigenous harvesting methods beyond high-level descriptions\n",
        "\n",
        "- No highly technical scientific literature or biochemical nutrition details\n",
        "\n",
        "**These omissions are intentional to ensure the chatbot remains safe, general, non-diagnostic, and aligned with government public-health guidance.**\n",
        "\n",
        "\n",
        "## **How We Improved the Raw Dataset**\n",
        "\n",
        "The raw text from websites and PDFs contained significant noise and had to be improved for RAG use. We enhanced the dataset by:\n",
        "\n",
        "- Removing menus, navigation bars, sidebars, advertisements, and irrelevant interface text\n",
        "\n",
        "- Removing duplicate sections, repeated headings, and broken formatting\n",
        "\n",
        "- Converting long lists and bullet points into clear sentences\n",
        "\n",
        "- Paraphrasing complex passages for clarity and readability\n",
        "\n",
        "- Combining related ideas and splitting overly long sections into smaller topic files\n",
        "\n",
        "- Ensuring every file contains clean, self-contained paragraphs for better embedding\n",
        "\n",
        "- Eliminating unsafe content such as personalized diet claims, prescriptive meal plans, and medical advice\n",
        "\n",
        "- Ensuring all text follows the same tone: neutral, factual, and educational\n",
        "\n",
        "These improvements make the dataset cleaner, more consistent, and much more effective for retrieval-augmented question answering.\n",
        "\n",
        "## **Does the Dataset Have Any Biases?**\n",
        "\n",
        "Yes, like all real-world datasets, this dataset has some inherent biases:\n",
        "\n",
        "1. **Institutional Bias:** Most content comes from government and public-health organizations, which follow Western nutrition frameworks and may not represent all cultural eating patterns.\n",
        "\n",
        "2. **Population Bias:** Guidelines are written for the general population, not for individuals with medical conditions, allergies, or special dietary needs.\n",
        "\n",
        "3. **Cultural Bias:** Though some cultural foods are mentioned, most recommendations reflect Canadian dietary norms and may not fully reflect global diversity.\n",
        "\n",
        "4. **Socioeconomic Bias:** Many recommendations assume consistent access to fresh foods, which may not be affordable or available to everyone.\n",
        "\n",
        "5. **Safety Bias:** To avoid harmful output, the dataset excludes weight-loss diets, restrictive eating patterns, and high-risk nutrition advice, which means the model cannot answer detailed personalized questions.\n",
        "\n",
        "We acknowledge these biases and mitigate their impact by adding safety guardrails, ensuring the chatbot gives general guidance only, and refuses personalized or clinical advice."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p9mESouVjcHj"
      },
      "source": [
        "# **Guardrails Implemented in Our Model**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sq6d96BwjfUl"
      },
      "source": [
        "We implemented several guardrails to ensure that NutriGuide provides safe, non-medical, and evidence-based responses. These restrictions prevent the model from giving harmful or overly personalized nutrition advice and ensure that all answers are grounded in the dataset rather than generated freely.\n",
        "\n",
        "\n",
        "1. **No Medical or Diagnostic Advice**\n",
        "\n",
        "The model is instructed to avoid: diagnosing health conditions, recommending treatments, interpreting symptoms, and giving clinical nutrition guidance (e.g., “What should a diabetic eat?”)\n",
        "\n",
        "If users ask medical questions, the chatbot responds with a safe refusal and encourages them to consult a health professional.\n",
        "\n",
        "2. **No Personalized Diet Plans**\n",
        "\n",
        "To reduce safety risks and avoid misinformation, the model will not provide calorie targets, macro calculations, weight-loss plans, and perfect diets based on age, sex, or weight.\n",
        "\n",
        "Instead, it offers general healthy-eating principles based on the dataset.\n",
        "\n",
        "3. **No Restrictive or Trend Diet Guidance**\n",
        "\n",
        "The model avoids recommending high-risk or restrictive diets such as:keto, fasting, detoxes, low-carb or extreme programs.\n",
        "\n",
        "If the user asks about these, the model explains general concepts but does not encourage restrictive eating.\n",
        "\n",
        "4. **Grounded Responses Only**\n",
        "\n",
        "All answers must come from the retrieved dataset. We enforce this by: inserting retrieved evidence directly into the prompt, instructing the model to answer ONLY using provided context, preventing unsupported claims or invented facts.\n",
        "\n",
        "If the context does not contain an answer, the model says it does not have information on that topic.\n",
        "\n",
        "5. **Safety-First Prompt Template**\n",
        "\n",
        "We added safety rules inside the system prompt, including:\n",
        "\n",
        "“Do not give medical advice.”\n",
        "\n",
        "“Do not create personalized diet plans.”\n",
        "\n",
        "“Use only the retrieved text to answer.”\n",
        "\n",
        "“If a question is unsafe or outside scope, politely decline.”\n",
        "\n",
        "This ensures consistency across all outputs.\n",
        "\n",
        "6. **Handling Sensitive Topics (eating disorders, weight loss, pregnancy)**\n",
        "\n",
        "For sensitive nutrition topics:the model avoids giving instructions, responses focus on general education, the model suggests speaking with a qualified professional.\n",
        "\n",
        "This protects users who may be vulnerable.\n",
        "\n",
        "7. **Filters Against Harmful Outputs**\n",
        "\n",
        "We added logic to detect high-risk prompts such as:extreme dieting, unsafe weight-loss methods,nutrient megadosing.\n",
        "\n",
        "In these cases, the chatbot provides a safety message instead of an answer.\n",
        "\n",
        "8. **No Storage of Personal Information**\n",
        "\n",
        "User messages are processed only to generate a response.\n",
        "We do not store:personal health data, demographic details, and user history.\n",
        "\n",
        "This reduces privacy risks."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JM8duudxn1dO"
      },
      "source": [
        "# **Install Dependencies**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h3Oehfn7oA2p"
      },
      "source": [
        "Install libraries required for the project:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "aYUNmkwNoGlQ"
      },
      "outputs": [],
      "source": [
        "!pip install transformers sentence-transformers -q"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0ZNEGjT0oSDK"
      },
      "source": [
        "# **Import Necessary Libraries**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "v4W9ZbwuoacK"
      },
      "outputs": [],
      "source": [
        "from transformers import pipeline\n",
        "from sentence_transformers import SentenceTransformer\n",
        "import pandas as pd\n",
        "import numpy as np"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pVQkMWxZpM8u"
      },
      "source": [
        "# **Google Drive**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RPzItbvYrcq3"
      },
      "source": [
        "we mounted Google Drive in Colab"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pl6yLiFirlht"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "\n",
        "# We mount our Google Drive so we can read the nutrition dataset from it\n",
        "drive.mount('/content/drive')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8HA2hbigt2C7"
      },
      "source": [
        "**What we did and why:**\n",
        "\n",
        "To manage our dataset efficiently, we stored all our cleaned nutrition text files inside a Google Drive folder and mounted that Drive into our Colab notebook. Mounting Google Drive allows the notebook to access files persistently, even across sessions, instead of uploading data manually every time. Once Drive was mounted, we specified the path to our dataset folder (/content/drive/MyDrive/NutriGuide_RAG/data) so the notebook could read all .txt files directly from storage. This setup ensures that our RAG system always has access to the latest version of our nutrition dataset."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f9uqGgIItwBw"
      },
      "source": [
        "# **Verifying and Accessing the Dataset Directory**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "q4lDGQvMsrM1"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import glob\n",
        "\n",
        "# We point to the data folder stored on Google Drive\n",
        "DATA_DIR = \"/content/drive/MyDrive/NutriGuide_RAG/data\"\n",
        "\n",
        "print(\"DATA_DIR:\", DATA_DIR)\n",
        "print(\"Files in DATA_DIR:\")\n",
        "print(glob.glob(os.path.join(DATA_DIR, \"*.txt\")))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BkNfNvVBuCpe"
      },
      "source": [
        "**What we did and why:**\n",
        "\n",
        "After mounting Drive, we confirmed that Python could correctly access the dataset folder by printing out all .txt files inside the directory. This step allowed us to verify that our cleaned documents were properly uploaded, that the file path was correct, and that the system could read the data before building embeddings. This check helped us catch potential mistakes early, such as missing files, incorrect folder names, or unreadable text formats."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OVW7d1aDuQIe"
      },
      "source": [
        "# **Loading the Nutrition Documents**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Dg7QksbBtbJe"
      },
      "outputs": [],
      "source": [
        "# We load all our cleaned nutrition documents from the data folder on Google Drive\n",
        "def load_nutrition_docs(folder=DATA_DIR):\n",
        "    docs = []\n",
        "    file_paths = sorted(glob.glob(os.path.join(folder, \"*.txt\")))\n",
        "\n",
        "    print(\"Found\", len(file_paths), \"files in\", folder)\n",
        "    for path in file_paths:\n",
        "        with open(path, \"r\", encoding=\"utf-8\") as f:\n",
        "            text = f.read().strip()\n",
        "        if text:\n",
        "            docs.append(text)\n",
        "        else:\n",
        "            print(\"Skipped empty file:\", path)\n",
        "    return docs\n",
        "\n",
        "# We call our loader function to get a list of all text documents\n",
        "documents = load_nutrition_docs(DATA_DIR)\n",
        "print(\"Number of non-empty documents:\", len(documents))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2w5rL0IouWVO"
      },
      "source": [
        "**What we did and why:**\n",
        "\n",
        "We created a custom function, load_nutrition_docs(), to automatically load every .txt file from our dataset folder. Instead of manually copying text into the notebook, this function scans the directory, opens each file, removes extra whitespace, and stores the cleaned text into a list called documents. This approach ensures consistency and scalability—if we add or update any nutrition files in the data folder, the notebook will automatically load the new content without requiring code changes. This loading step serves as the foundation for building embeddings and performing semantic retrieval in later stages."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MqJQeA54ul8u"
      },
      "source": [
        "# **Step 1: Ask LLM Without RAG**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eQ0bq5nLDWMh"
      },
      "outputs": [],
      "source": [
        "from transformers import pipeline\n",
        "\n",
        "# We load a lightweight pre-trained language model that can generate text.\n",
        "# This model answers questions only from its pretraining and NOT from our nutrition dataset.\n",
        "llm = pipeline('text-generation', model='gpt2')\n",
        "\n",
        "# We define a simple baseline question to show how the model behaves without RAG.\n",
        "question = \"What is healthy eating?\"\n",
        "\n",
        "# We ask the model directly, without retrieval, to get our baseline output.\n",
        "response = llm(question, max_length=80, num_return_sequences=1)\n",
        "\n",
        "# We print the raw answer from the model.\n",
        "print(\"Response from LLM WITHOUT RAG:\", response[0]['generated_text'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Vgm9Tq0ov3be"
      },
      "source": [
        "**What we did and why:**\n",
        "\n",
        "In this step, we tested how a pre-trained language model behaves before integrating our curated nutrition dataset. We loaded GPT-2 as a lightweight generator and asked it a simple nutrition question without giving it any supporting context. This allowed us to establish a baseline for comparison. Because the model answered purely from its general pretraining, the response was generic and did not reference Canada’s Food Guide, WHO guidelines, or any evidence-based recommendations. Observing these limitations demonstrated why retrieval was necessary: without RAG, the model cannot provide grounded, trustworthy, or Canada-specific nutrition guidance."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qNU-3e2_wR_1"
      },
      "source": [
        "# **Step 2: Create dense embeddings for our nutrition documents**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AsxvGGp0xW03"
      },
      "source": [
        "## **Prepare a Dataset**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "r2svYmkKyCMO"
      },
      "outputs": [],
      "source": [
        "documents = load_nutrition_docs(DATA_DIR)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0dmCXEF6y1Zc"
      },
      "outputs": [],
      "source": [
        "# Step 2: Prepare a Dataset (Knowledge Base)\n",
        "# In our project, 'documents' is already defined from our nutrition txt files.\n",
        "# It contains cleaned text from Canada's Food Guide, WHO, and Heart & Stroke.\n",
        "print(\"Number of documents in our nutrition corpus:\", len(documents))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zQXODh1jzhJo"
      },
      "source": [
        "## **Create dense embeddings**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qSwaMFnkzKZh"
      },
      "outputs": [],
      "source": [
        "from sentence_transformers import SentenceTransformer\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "# We use MiniLM to embed each document into a dense vector for similarity search\n",
        "embedding_model = SentenceTransformer('all-MiniLM-L6-v2')\n",
        "\n",
        "embeddings = embedding_model.encode(documents)  # Generate embeddings for all docs\n",
        "\n",
        "# We create a pandas DataFrame that links each document to its embedding\n",
        "df = pd.DataFrame({\n",
        "    \"Document\": documents,\n",
        "    \"Embedding\": list(embeddings)  # store each embedding as a list\n",
        "})\n",
        "\n",
        "print(\"Indexed Documents (first 3 rows):\")\n",
        "print(df.head(3))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LOjeYoFLwR_4"
      },
      "source": [
        "**What we did and why:**\n",
        "\n",
        "We used the MiniLM sentence embedding model to convert each nutrition document into a dense vector and stored both the raw text and its embedding in a pandas DataFrame, which serves as a simple, searchable index for our RAG system."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C6GzhZCB0F9B"
      },
      "source": [
        "## **Define a retrieval function**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "h6NNLdOUz7TB"
      },
      "outputs": [],
      "source": [
        "# we defined a retrieval function that finds the most relevant nutrition documents\n",
        "def retrieve_with_pandas(query, top_k=2):\n",
        "    \"\"\"\n",
        "    We embed the user's query and compute cosine similarity with each document embedding.\n",
        "    Then we return the top-k most similar nutrition documents.\n",
        "    \"\"\"\n",
        "    # Generate embedding for the query\n",
        "    query_embedding = embedding_model.encode([query])[0]\n",
        "\n",
        "    # Compute cosine similarity between query and each document embedding\n",
        "    def cosine_sim(vec):\n",
        "        vec = np.array(vec)\n",
        "        return np.dot(query_embedding, vec) / (np.linalg.norm(query_embedding) * np.linalg.norm(vec))\n",
        "\n",
        "    df['Similarity'] = df['Embedding'].apply(cosine_sim)\n",
        "\n",
        "    # Sort by similarity and return top-k results\n",
        "    results = df.sort_values(by=\"Similarity\", ascending=False).head(top_k)\n",
        "    return results[[\"Document\", \"Similarity\"]]\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g_9VPwFO0I5T"
      },
      "source": [
        "**What we did and why:**\n",
        "\n",
        "To retrieve relevant context, we implemented retrieve_with_pandas(), which embeds the user’s question and computes cosine similarity between the query embedding and each document embedding in the DataFrame. The function returns the top-k nutrition documents with the highest similarity scores, forming the context we later pass to the language model."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aZxsBxDN0Q-2"
      },
      "source": [
        "## **RAG function using your llm pipeline**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6eQN5bv40SaG"
      },
      "outputs": [],
      "source": [
        "# Define a simple RAG function that uses retrieved context + LLM\n",
        "def generate_with_rag(query, top_k=2):\n",
        "    \"\"\"\n",
        "    We retrieve the most relevant nutrition documents using our index\n",
        "    and then ask the language model to answer the question using that context.\n",
        "    \"\"\"\n",
        "    # Retrieve top-k relevant documents\n",
        "    results = retrieve_with_pandas(query, top_k=top_k)\n",
        "    context_texts = list(results[\"Document\"])\n",
        "\n",
        "    # Combine the context into a single block of text\n",
        "    context_block = \"\\n\\n\".join(context_texts)\n",
        "\n",
        "    # Build the input prompt for the LLM\n",
        "    input_text = (\n",
        "        \"You are NutriGuide, a safe nutrition assistant. \"\n",
        "        \"Use ONLY the context below to answer the question. \"\n",
        "        \"Do NOT give medical advice or personalized meal plans.\\n\\n\"\n",
        "        f\"Context:\\n{context_block}\\n\\n\"\n",
        "        f\"Question: {query}\\n\\nAnswer in a short, clear paragraph:\"\n",
        "    )\n",
        "\n",
        "    # Use our LLM (gpt2 pipeline defined earlier) to generate a response\n",
        "    response = llm(input_text, max_length=200, num_return_sequences=1)\n",
        "    return response[0]['generated_text']\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VRrfxaei0YsW"
      },
      "source": [
        "**What we did and why:**\n",
        "\n",
        "Our generate_with_rag() function first calls retrieve_with_pandas() to obtain the most relevant nutrition documents, merges them into a single context block, and then feeds this context together with the user’s question into the GPT-2 language model. The prompt explicitly instructs the model to answer only using the provided context and to avoid medical or highly personalized advice. This mirrors the teacher’s RAG example but is adapted to our curated nutrition dataset and safety constraints."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rQB0soM70eCF"
      },
      "source": [
        "## **Test query**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "81A7hCGa0jMl"
      },
      "outputs": [],
      "source": [
        "# Test the retrieval and RAG generation on a nutrition-related question\n",
        "query = \"What are some key recommendations for healthy eating?\"\n",
        "\n",
        "print(\"\\nTop Retrieved Documents:\")\n",
        "results = retrieve_with_pandas(query, top_k=2)\n",
        "print(results)\n",
        "\n",
        "print(\"\\nRAG Answer:\")\n",
        "print(generate_with_rag(query, top_k=2))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GYDbAaFe1fT_"
      },
      "source": [
        "# **Step 3: Ask Question with RAG**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9eXak3hL2N7N"
      },
      "outputs": [],
      "source": [
        "# Ask the same question again, but this time USING RAG\n",
        "\n",
        "# We reuse the same 'question' variable we used in the baseline test\n",
        "print(\"Original question:\", question)\n",
        "\n",
        "# We now generate an answer using our RAG pipeline (retrieval + generation)\n",
        "response_rag = generate_with_rag(question)\n",
        "\n",
        "print(\"Response from LLM WITH RAG:\")\n",
        "print(response_rag)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5Fpsqbhr2vsv"
      },
      "source": [
        "\n",
        "**What we did and why:**\n",
        "\n",
        "To evaluate the impact of retrieval, we asked the language model the same question twice: first without RAG, and then using our generate_with_rag() pipeline. In the baseline setting, the model answered purely from its pretraining and produced a long, generic description of “healthy eating” that was not clearly tied to any guideline. In contrast, the RAG version first retrieved relevant passages from our curated nutrition dataset and then generated an answer based on that context. This second answer was more focused, aligned with public health recommendations, and avoided hallucinated or unnecessary details, showing that retrieval significantly improved the grounding and quality of responses."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pTne6sA83QUE"
      },
      "source": [
        "# **Step 4: Compare the Results**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ipgrp7i93qhR"
      },
      "outputs": [],
      "source": [
        "# we compared the Baseline vs RAG Responses\n",
        "\n",
        "print(\"=== WITHOUT RAG (Baseline LLM Response) ===\")\n",
        "print(response[0]['generated_text'])\n",
        "print(\"\\n****\\n\")\n",
        "\n",
        "print(\"=== WITH RAG (NutriGuide RAG Response) ===\")\n",
        "print(response_rag)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yD3J7p6G3Tmu"
      },
      "source": [
        "**What we did and why:**\n",
        "\n",
        "To evaluate the effect of retrieval on answer quality, we asked the same question using two different methods: (1) the language model alone, and (2) our NutriGuide RAG system. The baseline GPT-2 model produced a vague and repetitive answer that relied solely on its general pretraining and did not reference any nutrition guidelines. In contrast, the RAG-enhanced version retrieved relevant information from our curated nutrition dataset and generated a response grounded in evidence-based sources such as Canada’s Food Guide and WHO recommendations. The RAG answer was clearer, more accurate, and aligned with public-health principles, demonstrating a clear improvement in safety, relevance, and grounding."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7cEgnr8t5q4P"
      },
      "source": [
        "# **Changes to the Model**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1bZDP21853QK"
      },
      "source": [
        "**Did you need to change the model in any way before making it application ready, or fine-tune it?**\n",
        "\n",
        "In our project, we did not change the underlying weights of the language model or fine-tune it. Instead, we kept the pre-trained DistilGPT-2 model as-is and made it application ready by wrapping it inside a RAG pipeline and adding strong prompt-level guardrails.\n",
        "\n",
        "The main changes we made were:\n",
        "\n",
        "- Restricting the model to short, grounded answers using a custom prompt template.\n",
        "\n",
        "- Instructing it to only use information from the retrieved nutrition context.\n",
        "\n",
        "- Adding explicit safety rules (no medical advice, no calorie targets, no personalized diet plans, no extreme diets).\n",
        "\n",
        "This approach allowed us to adapt the model to our nutrition chatbot use case without fine-tuning, which is safer and more appropriate for a small course project dealing with health-related content."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U1R9LhM27oeV"
      },
      "source": [
        "# **Data Responses**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8yT7Jnbi-sQj"
      },
      "source": [
        "## **How we receive the data from the user**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "90PB5zWu7xz3"
      },
      "source": [
        "\n",
        "\n",
        "For our application, the user interacts with NutriGuide through a simple text-based chat interface (for now, this is implemented in a notebook cell / web-style input box). The user types a natural-language question like “How can I eat more vegetables?” or “What are healthier drink choices?” and we pass that string into our RAG pipeline as the query.\n",
        "\n",
        "We designed the assistant so that it does not need detailed personal information (exact weight, diagnoses, medications, etc.) in order to give useful, general nutrition guidance. In a more polished UI, we would also include a short note asking users not to enter sensitive medical or identity data.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UKbxa9Pn-zr3"
      },
      "source": [
        "## **How the data is cleaned after it is received**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Lqtj2x7b9lfD"
      },
      "source": [
        "\n",
        "\n",
        "\n",
        "Once the user submits a question, we apply light, safe preprocessing before using it:\n",
        "\n",
        "- Trim leading/trailing whitespace\n",
        "\n",
        "- Drop completely empty or whitespace-only inputs\n",
        "\n",
        "- Optionally normalize spacing and basic punctuation\n",
        "\n",
        "- We do not change the meaning of the input; we just standardize it enough so embeddings and retrieval work reliably.\n",
        "\n",
        "On top of this, the guardrails in our prompt act as a second layer of cleaning at the meaning level:\n",
        "\n",
        "- Even if the user includes age, gender, or medical details, we instruct the model not to give personalized meal plans, calories, or diagnoses.\n",
        "\n",
        "- If a question clearly requires medical or treatment advice, the assistant is instructed to refuse and recommend talking to a health professional instead of using that information to generate a risky answer.\n",
        "\n",
        "So, we keep text preprocessing minimal, but we enforce strong semantic safety through the system prompt and RAG design.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JE2cUFtN_CwV"
      },
      "source": [
        "## **How and where the data is stored**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZaKFP7Kq9p8M"
      },
      "source": [
        "\n",
        "\n",
        "\n",
        "\n",
        "For this course project, we do not persist real user data in any external database:\n",
        "- Inputs are held only in memory inside the running notebook session to generate a single response.\n",
        "- If we log example conversations, they are synthetic test prompts we wrote ourselves (e.g., “a student wants cheaper healthy snacks”) and are not tied to any user identity.\n",
        "\n",
        "In a real deployment scenario, we would:\n",
        "\n",
        "- Store only anonymized logs of queries (no names, no contact info, no direct identifiers).\n",
        "\n",
        "- Use them only for debugging and evaluation (e.g., checking if the model followed guardrails).\n",
        "\n",
        "- Host logs in a secure backend with access control and a clear privacy policy.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z1DsBk4M_JSq"
      },
      "source": [
        "## **Is the cleaning process the same in all environments?**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PiRpy1yF9sUg"
      },
      "source": [
        "\n",
        "\n",
        "\n",
        "In our current setup (notebook / simple web-style interface), yes the same cleaning process applies to all users:\n",
        "\n",
        "- Strip whitespace, ignore empty queries\n",
        "- Send the cleaned text to the embedding model and retriever\n",
        "- Rely on the same guardrails and refusal rules for safety\n",
        "\n",
        "If NutriGuide were deployed across different environments (for example, a web app, mobile app, or voice assistant), the core cleaning logic would stay the same (normalize text, avoid storing sensitive information, enforce guardrails). However, we would add channel-specific steps:\n",
        "\n",
        "**For voice input:** transcribe speech to text, then run the same cleaning.\n",
        "\n",
        "**For messaging apps:** normalize emojis, short forms, or remove attachment placeholders, then process as usual.\n",
        "\n",
        "But in all cases, our design principle stays consistent:\n",
        "- minimal technical cleaning,\n",
        "- no need for personal identifiers,\n",
        "- safety enforced at the prompt/behavior level rather than by collecting more data."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2RzW0Ufgh21-"
      },
      "source": [
        "# **Challenges and ethical and bias issues**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bDfCOWIvKCwS"
      },
      "source": [
        "**What are the challenges?**\n",
        "\n",
        "One of the main challenges in our project is that users often expect personalized diet plans, medical advice, or weight-loss instructions when interacting with a nutrition chatbot. These are unsafe for an AI model to provide. Another challenge is that nutrition questions can be very broad or vague, making it difficult for the model to determine what the user is actually asking. We also had to manage the technical challenge of making sure the model retrieves the correct information from our dataset instead of hallucinating.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q1T9GX3OK9zh"
      },
      "source": [
        "One of the biggest challenges we faced while creating this project was preparing the dataset for RAG. The sources we used (Canada’s Food Guide, WHO documents, Heart & Stroke Foundation) were extremely long, repetitive, and formatted with bullet points, lists, headings, and duplicated sections. We had to manually clean, merge, and rewrite large parts of the text to make it usable. Another challenge was chunking the data properly so the model could retrieve meaningful sections without cutting sentences or losing important context.\n",
        "\n",
        "We also struggled with technical issues inside Google Colab, such as metadata errors, widget states breaking the notebook, long loading times, and memory issues when generating embeddings. Making sure Google Drive connected properly and ensuring the dataset loaded correctly in the notebook took multiple attempts. Finally, customizing the instructor’s notebook to fit our project required rewriting functions, adding safety guardrails, updating prompts, and changing the retrieval system so the model only uses our nutrition dataset and not the original sample text."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VPOdbPWXKUVp"
      },
      "source": [
        "**What ethical and bias issues will users be facing?**\n",
        "\n",
        "Users may face ethical risks such as accidentally receiving information that feels medical when it is not intended to be. Our dataset is based on Canadian and WHO sources, which means it may not reflect all cultural diets, food traditions, or regional foods. This can create bias toward Western healthy-eating patterns. Some users may also rely too heavily on the tool, assuming it can replace a dietitian or a healthcare professional, which is not the case."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gfhDXShcKY2r"
      },
      "source": [
        "**How did you address these issues in your application?**\n",
        "\n",
        "To address safety and ethics, we added strict guardrails that prevent the model from giving personalized, medical, or diagnostic advice. The chatbot avoids calorie numbers, supplement dosages, and disease-specific diet plans, and instead provides general guidance only. We also implemented refusal messages for unsafe topics and remind users to consult professionals. To reduce cultural bias, we structured answers around broad healthy-eating principles instead of specific recipes or culture-specific foods. Finally, using RAG ensures that all answers come only from trusted, verified sources in our dataset."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nBnQK4PhKiXc"
      },
      "source": [
        "**What are the edge cases your model might fail?**\n",
        "\n",
        "Our model may struggle or fail in cases where users ask for:\n",
        "\n",
        "- medical diet plans (e.g., diabetes diets, heart-disease diets)\n",
        "\n",
        "- extremely personalized instructions (e.g., “Tell me what to eat every hour”)\n",
        "\n",
        "- weight-loss targets or calorie restrictions\n",
        "\n",
        "- information not included in our dataset (e.g., TikTok diet trends)\n",
        "\n",
        "- questions with multiple unrelated parts\n",
        "\n",
        "- cultural foods or practices the dataset does not mention\n",
        "\n",
        "In these cases, the model may not retrieve relevant text or may risk hallucinating."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ko7qedHOK3xl"
      },
      "source": [
        "**How can you address these?**\n",
        "\n",
        "We address these failures by designing safe fallback responses. If the model cannot find information in the dataset, it tells the user that the information is unavailable. If the question is medical or unsafe, the model gives a refusal message and redirects the user to a professional. For vague or multi-part questions, we guide the user to clarify what they mean. In future versions, adding more diverse datasets, expanding cultural food sources, and fine-tuning the model on safer dialog patterns would further reduce these edge-case failures."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TvO7qcSLQdLW"
      },
      "source": [
        "# **Feedback**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hD5KJEvNMi6E"
      },
      "source": [
        "# **Links**\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tRomqrkXMmx7"
      },
      "source": [
        "Github:\n",
        "Google drive: https://drive.google.com/drive/folders/1oOSY-QsMjhM8ZkDPHsxS54dM0m7gGCwW?usp=sharing\n",
        "  "
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "URt4Q-TNmNDj",
        "4Qpcib1lo92R",
        "L3FGskainy8v",
        "z_jGywr11D9R",
        "A241w5TIUs--",
        "p9mESouVjcHj",
        "JM8duudxn1dO",
        "0ZNEGjT0oSDK",
        "pVQkMWxZpM8u",
        "f9uqGgIItwBw",
        "OVW7d1aDuQIe",
        "MqJQeA54ul8u",
        "AsxvGGp0xW03",
        "zQXODh1jzhJo",
        "C6GzhZCB0F9B",
        "aZxsBxDN0Q-2",
        "rQB0soM70eCF",
        "GYDbAaFe1fT_",
        "pTne6sA83QUE",
        "7cEgnr8t5q4P",
        "U1R9LhM27oeV",
        "8yT7Jnbi-sQj",
        "UKbxa9Pn-zr3",
        "JE2cUFtN_CwV",
        "Z1DsBk4M_JSq",
        "2RzW0Ufgh21-"
      ],
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}